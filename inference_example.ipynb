{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import ipywidgets as ipyw\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchio as tio\n",
    "\n",
    "from utils.geodis_toolkits import geodismap\n",
    "from models.networks import P_RNet3D\n",
    "from data_loaders.transforms import get_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pnet_inference(\n",
    "    image_path,\n",
    "    save_path,\n",
    "    pnet, \n",
    "    transform, \n",
    "    device\n",
    "):\n",
    "    \"\"\"\n",
    "    P-Net inference function\n",
    "    \n",
    "    Args:\n",
    "        image_path: file path of input image (ex. image_path.nii.gz)\n",
    "        save_path:  file path to save result (ex. pnet_pred.nii.gz)\n",
    "        pnet:       trained pnet model (torch.nn.Module)\n",
    "        transform:  preprocessing transforms (torchio.Compose)\n",
    "        device:     torch device (torch.device)\n",
    "    \"\"\"\n",
    "\n",
    "    # read image and make subject to apply transform\n",
    "    subject = tio.Subject(\n",
    "        image = tio.ScalarImage(image_path),\n",
    "    )\n",
    "    subject = transform(subject)\n",
    "\n",
    "    # make numpy array to torch tensor\n",
    "    input_image = subject.image.data\n",
    "    input_tensor = input_image.unsqueeze(dim=0).to(device)\n",
    "\n",
    "    # inference\n",
    "    with torch.no_grad():\n",
    "        pred_logits = pnet(input_tensor)\n",
    "    \n",
    "    # logits to labels\n",
    "    pred_labels = torch.argmax(pred_logits, dim=1)\n",
    "\n",
    "    # labels to one hot labels (ex. [1, 2, 3] -> [[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "    pred_onehot = torch.nn.functional.one_hot(pred_labels, 2).permute(0, 4, 1, 2, 3)\n",
    "    pred_onehot_target = pred_onehot[:, 1, ...]\n",
    "\n",
    "    # save result\n",
    "    pred_labelmap = tio.LabelMap(tensor=pred_onehot_target.cpu())\n",
    "    print(pred_labelmap.shape)\n",
    "    pred_labelmap.save(save_path)\n",
    "\n",
    "    print(f\"Saved pnet result: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnet_inference(\n",
    "    image_path, \n",
    "    pnet_pred_path,\n",
    "    fg_point_path, \n",
    "    bg_point_path, \n",
    "    save_path,\n",
    "    rnet, \n",
    "    transform, \n",
    "    device\n",
    "):\n",
    "    \"\"\"\n",
    "    R-Net inference function\n",
    "    \n",
    "    Args:\n",
    "        image_path:     file path of input image (ex. image_path.nii.gz)\n",
    "        pnet_pred_path: file path of pnet prediction label (ex. pnet_pred.nii.gz)\n",
    "        fg_point_path:  foreground user interaction points file path (ex. fg_points.npy)\n",
    "        bg_point_path:  background user interaction points file path (ex. bg_points.npy)\n",
    "        save_path:      file path to save result (ex. pnet_pred.nii.gz)\n",
    "        rnet:           trained rnet model (torch.nn.Module)\n",
    "        transform:      preprocessing transforms (torchio.Compose)\n",
    "        device:         torch device (torch.device)\n",
    "    \"\"\"\n",
    "\n",
    "    # read image and pnet prediction and make subject to apply transform\n",
    "    subject = tio.Subject(\n",
    "        image = tio.ScalarImage(image_path),\n",
    "        pnet_pred = tio.LabelMap(pnet_pred_path)\n",
    "    )\n",
    "    subject = transform(subject)\n",
    "\n",
    "    # cast numpy array to torch tensor\n",
    "    input_image = subject.image.data\n",
    "    input_tensor = input_image.unsqueeze(dim=0).to(device)\n",
    "\n",
    "    pnet_pred_label = subject.pnet_pred.data\n",
    "    pnet_pred_tensor = pnet_pred_label.unsqueeze(dim=0).to(device)\n",
    "\n",
    "    # read random point numpy array\n",
    "    sf, sb = np.load(fg_point_path), np.load(bg_point_path)\n",
    "\n",
    "    # get geodismap from random points and apply transform\n",
    "    sf, sb = sf.astype(np.float32), sb.astype(np.float32)\n",
    "    fore_dist_map, back_dist_map = geodismap(sf, sb, image_path)\n",
    "    fore_dist_map = torch.Tensor(transform(np.expand_dims(fore_dist_map.transpose(1, 2, 0), axis=0)))\n",
    "    back_dist_map = torch.Tensor(transform(np.expand_dims(back_dist_map.transpose(1, 2, 0), axis=0)))\n",
    "\n",
    "    # make rnet input tensor\n",
    "    rnet_inputs = torch.cat([\n",
    "        input_tensor,\n",
    "        pnet_pred_tensor, \n",
    "        fore_dist_map.unsqueeze(dim=1).to(device), \n",
    "        back_dist_map.unsqueeze(dim=1).to(device)\n",
    "    ], dim=1)\n",
    "\n",
    "    # inference\n",
    "    with torch.no_grad():\n",
    "        pred_logits = rnet(rnet_inputs)\n",
    "    \n",
    "    # logits to labels\n",
    "    pred_labels = torch.argmax(pred_logits, dim=1)\n",
    "\n",
    "    # labels to one hot labels (ex. [1, 2, 3] -> [[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "    pred_onehot = torch.nn.functional.one_hot(pred_labels, 2).permute(0, 4, 1, 2, 3)\n",
    "    pred_onehot_target = pred_onehot[:, 1, ...]\n",
    "\n",
    "    # save result\n",
    "    pred_labelmap = tio.LabelMap(tensor=pred_onehot_target.cpu())\n",
    "    pred_labelmap.save(save_path)\n",
    "\n",
    "    print(f\"Saved rnet result: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Inference on test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test file paths\n",
    "test_dir = \"./dataset/test\"\n",
    "test_images = glob.glob(f\"{test_dir}/*/*_flair.nii.gz\")\n",
    "test_labels = glob.glob(f\"{test_dir}/*/*_seg.nii.gz\")\n",
    "\n",
    "# save directory\n",
    "save_dir = \"./results\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# preprocessing\n",
    "test_transform = get_transform(\"valid\")\n",
    "\n",
    "# device config\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "P_RNet3D(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv3d(4, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv3d(16, 16, kernel_size=(3, 3, 1), stride=(1, 1, 1), padding=(1, 1, 0))\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (block1_downsample): Sequential(\n",
       "    (0): Conv3d(16, 4, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(2, 2, 2), dilation=(2, 2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): Conv3d(16, 16, kernel_size=(3, 3, 1), stride=(1, 1, 1), padding=(2, 2, 0), dilation=(2, 2, 1))\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (block2_downsample): Sequential(\n",
       "    (0): Conv3d(16, 4, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(4, 4, 4), dilation=(4, 4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv3d(16, 16, kernel_size=(3, 3, 1), stride=(1, 1, 1), padding=(4, 4, 0), dilation=(4, 4, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv3d(16, 16, kernel_size=(3, 3, 1), stride=(1, 1, 1), padding=(4, 4, 0), dilation=(4, 4, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (block3_downsample): Sequential(\n",
       "    (0): Conv3d(16, 4, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(8, 8, 8), dilation=(8, 8, 8))\n",
       "    (1): ReLU()\n",
       "    (2): Conv3d(16, 16, kernel_size=(3, 3, 1), stride=(1, 1, 1), padding=(8, 8, 0), dilation=(8, 8, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv3d(16, 16, kernel_size=(3, 3, 1), stride=(1, 1, 1), padding=(8, 8, 0), dilation=(8, 8, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (block4_downsample): Sequential(\n",
       "    (0): Conv3d(16, 4, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (block5): Sequential(\n",
       "    (0): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(16, 16, 16), dilation=(16, 16, 16))\n",
       "    (1): ReLU()\n",
       "    (2): Conv3d(16, 16, kernel_size=(3, 3, 1), stride=(1, 1, 1), padding=(16, 16, 0), dilation=(16, 16, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv3d(16, 16, kernel_size=(3, 3, 1), stride=(1, 1, 1), padding=(16, 16, 0), dilation=(16, 16, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (block5_downsample): Sequential(\n",
       "    (0): Conv3d(16, 4, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (block6): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv3d(20, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Conv3d(16, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model weight file paths\n",
    "pnet_best_ckpt_dir = \"./experiments/best_ckpts/brats3d_pnet_init_train\"\n",
    "pnet_best_ckpt_path = sorted(glob.glob(f\"{pnet_best_ckpt_dir}/*.pt\"))[-1]\n",
    "\n",
    "rnet_best_ckpt_dir = \"./experiments/best_ckpts/brats3d_rnet_init_train\"\n",
    "rnet_best_ckpt_path = sorted(glob.glob(f\"{rnet_best_ckpt_dir}/*.pt\"))[-1]\n",
    "\n",
    "# load model weights\n",
    "pnet = P_RNet3D(c_in=1, c_blk=16, n_classes=2).to(device)\n",
    "rnet = P_RNet3D(c_in=4, c_blk=16, n_classes=2).to(device)\n",
    "\n",
    "pnet.load_state_dict(torch.load(pnet_best_ckpt_path))\n",
    "rnet.load_state_dict(torch.load(rnet_best_ckpt_path))\n",
    "\n",
    "pnet.eval()\n",
    "rnet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 240, 240, 155)\n",
      "Saved pnet result: ./results/BraTS2021_01647_pred_pnet.nii.gz\n"
     ]
    }
   ],
   "source": [
    "# P-Net inference\n",
    "\n",
    "# Ex) test/BraTS2021_01647/BraTS2021_01647_flair.nii.gz -> BraTS2021_01647_pred_pnet.nii.gz\n",
    "save_path_pnet = os.path.join(save_dir, os.path.basename(test_images[0]).replace(\"_flair\", \"_pred_pnet\"))\n",
    "\n",
    "pnet_pred_labels = pnet_inference(image_path=test_images[0],\n",
    "                                  save_path=save_path_pnet,\n",
    "                                  pnet=pnet,\n",
    "                                  transform=test_transform,\n",
    "                                  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rnet result: ./results/BraTS2021_01647_pred_rnet.nii.gz\n"
     ]
    }
   ],
   "source": [
    "# R-Net inference\n",
    "\n",
    "# Ex) test/BraTS2021_01647/BraTS2021_01647_flair.nii.gz -> BraTS2021_01647_pred_rnet.nii.gz\n",
    "save_path_rnet = os.path.join(save_dir, os.path.basename(test_images[0]).replace(\"_flair\", \"_pred_rnet\"))\n",
    "\n",
    "rnet_pred_labels = rnet_inference(image_path=test_images[0],\n",
    "                                  pnet_pred_path=save_path_pnet,\n",
    "                                  fg_point_path=os.path.join(save_dir, \"int_pos_result.npy\"),\n",
    "                                  bg_point_path=os.path.join(save_dir, \"int_neg_result.npy\"),\n",
    "                                  save_path=save_path_rnet,\n",
    "                                  rnet=rnet,\n",
    "                                  transform=test_transform,\n",
    "                                  device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSliceViewer3D_with_prediction:\n",
    "    \n",
    "    # reference = https://github.com/esmitt/imageSliceViewer/blob/master/SliceViewer.ipynb\n",
    "    \n",
    "    \"\"\" \n",
    "    ImageSliceViewer3D is for viewing volumetric image slices in jupyter or\n",
    "    ipython notebooks. \n",
    "    \n",
    "    User can interactively change the slice plane selection for the image and \n",
    "    the slice plane being viewed. \n",
    "\n",
    "    Argumentss:\n",
    "    Volume = 3D input image\n",
    "    figsize = default(8,8), to set the size of the figure\n",
    "    cmap = default('gray'), string for the matplotlib colormap. You can find \n",
    "    more matplotlib colormaps on the following link:\n",
    "    https://matplotlib.org/users/colormaps.html\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image, label, pred_pnet, pred_rnet, overlap = False, figsize=(50,50), cmap_image='gray', cmap_label = 'viridis', cmap_pred_pnet = 'inferno', cmap_pred_rnet = 'magma'):\n",
    "        self.image = image\n",
    "        self.label = label \n",
    "        self.pred_pnet = pred_pnet\n",
    "        self.pred_rnet = pred_rnet\n",
    "        self.figsize = figsize\n",
    "        self.cmap_image = cmap_image\n",
    "        self.cmap_label = cmap_label\n",
    "        self.cmap_pred_pnet = cmap_pred_pnet\n",
    "        self.cmap_pred_rnet = cmap_pred_rnet\n",
    "        self.v = [np.min(image), np.max(image)]\n",
    "        self.v_label = [np.min(label), np.max(label)]\n",
    "        self.v_pred_pnet = [np.min(pred_pnet), np.max(pred_pnet)]\n",
    "        self.v_pred_rnet = [np.min(pred_rnet), np.max(pred_rnet)]\n",
    "        self.overlap = overlap\n",
    "\n",
    "        # Call to select slice plane\n",
    "        ipyw.interact(self.views)\n",
    "    \n",
    "    def views(self):\n",
    "        self.vol1 = np.transpose(self.image, [1,2,0])\n",
    "        self.vol2 = np.rot90(np.transpose(self.image, [2,0,1]), 3) #rotate 270 degrees\n",
    "        self.vol3 = np.transpose(self.image, [0,1,2])\n",
    "        maxZ1 = self.vol1.shape[2] - 1\n",
    "        maxZ2 = self.vol2.shape[2] - 1\n",
    "        maxZ3 = self.vol3.shape[2] - 1\n",
    "        \n",
    "        self.vol1_label = np.transpose(self.label, [1,2,0])\n",
    "        self.vol2_label = np.rot90(np.transpose(self.label, [2,0,1]), 3) #rotate 270 degrees\n",
    "        self.vol3_label = np.transpose(self.label, [0,1,2])\n",
    "        \n",
    "        self.vol1_pred_pnet = np.transpose(self.pred_pnet, [1,2,0])\n",
    "        self.vol2_pred_pnet = np.rot90(np.transpose(self.pred_pnet, [2,0,1]), 3) #rotate 270 degrees\n",
    "        self.vol3_pred_pnet = np.transpose(self.pred_pnet, [0,1,2])\n",
    "        \n",
    "        self.vol1_pred_rnet = np.transpose(self.pred_rnet, [1,2,0])\n",
    "        self.vol2_pred_rnet = np.rot90(np.transpose(self.pred_rnet, [2,0,1]), 3) #rotate 270 degrees\n",
    "        self.vol3_pred_rnet = np.transpose(self.pred_rnet, [0,1,2])\n",
    "        \n",
    "        ipyw.interact(self.plot_slice, \n",
    "            z1=ipyw.IntSlider(min=0, max=maxZ1, step=1, continuous_update=False, \n",
    "            description='Axial:'), \n",
    "            z2=ipyw.IntSlider(min=0, max=maxZ2, step=1, continuous_update=False, \n",
    "            description='Coronal:'),\n",
    "            z3=ipyw.IntSlider(min=0, max=maxZ3, step=1, continuous_update=False, \n",
    "            description='Sagittal:'))\n",
    "\n",
    "    def plot_slice(self, z1, z2, z3):\n",
    "        \n",
    "        if self.overlap:\n",
    "\n",
    "            f, ax = plt.subplots(1,3, figsize=self.figsize)\n",
    "            ax[0].imshow(self.vol1[:,:,z1], cmap=plt.get_cmap(self.cmap_image), \n",
    "                vmin=self.v[0], vmax=self.v[1])\n",
    "            ax[1].imshow(self.vol2[:,:,z2], cmap=plt.get_cmap(self.cmap_image), \n",
    "                vmin=self.v[0], vmax=self.v[1])\n",
    "            ax[2].imshow(self.vol3[:,:,z3], cmap=plt.get_cmap(self.cmap_image), \n",
    "                vmin=self.v[0], vmax=self.v[1])\n",
    "            \n",
    "            ax[0].imshow(self.vol1_label[:,:,z1], cmap=plt.get_cmap(self.cmap_label),\n",
    "                vmin=self.v_label[0], vmax=self.v_label[1], alpha = 0.3)\n",
    "            ax[1].imshow(self.vol2_label[:,:,z2], cmap=plt.get_cmap(self.cmap_label),\n",
    "                vmin=self.v_label[0], vmax=self.v_label[1], alpha = 0.3)\n",
    "            ax[2].imshow(self.vol3_label[:,:,z3], cmap=plt.get_cmap(self.cmap_label),\n",
    "                vmin=self.v_label[0], vmax=self.v_label[1], alpha = 0.3)\n",
    "            \n",
    "            ax[0].imshow(self.vol1_pred_pnet[:,:,z1], cmap=plt.get_cmap(self.cmap_pred_pnet),\n",
    "                vmin=self.v_pred_pnet[0], vmax=self.v_pred_pnet[1], alpha = 0.3)\n",
    "            ax[1].imshow(self.vol2_pred_pnet[:,:,z2], cmap=plt.get_cmap(self.cmap_pred_pnet),\n",
    "                vmin=self.v_pred_pnet[0], vmax=self.v_pred_pnet[1], alpha = 0.3)\n",
    "            ax[2].imshow(self.vol3_pred_pnet[:,:,z3], cmap=plt.get_cmap(self.cmap_pred_pnet),\n",
    "                vmin=self.v_pred_pnet[0], vmax=self.v_pred_pnet[1], alpha = 0.3)\n",
    "\n",
    "            ax[0].imshow(self.vol1_pred_rnet[:,:,z1], cmap=plt.get_cmap(self.cmap_pred_rnet),\n",
    "                vmin=self.v_pred_rnet[0], vmax=self.v_pred_rnet[1], alpha = 0.3)\n",
    "            ax[1].imshow(self.vol2_pred_rnet[:,:,z2], cmap=plt.get_cmap(self.cmap_pred_rnet),\n",
    "                vmin=self.v_pred_rnet[0], vmax=self.v_pred_rnet[1], alpha = 0.3)\n",
    "            ax[2].imshow(self.vol3_pred_rnet[:,:,z3], cmap=plt.get_cmap(self.cmap_pred_rnet),\n",
    "                vmin=self.v_pred_rnet[0], vmax=self.v_pred_rnet[1], alpha = 0.3)\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            # default settings: without overlap plots\n",
    "            f, ax = plt.subplots(4,3, figsize=self.figsize)\n",
    "            ax[0][0].imshow(self.vol1[:,:,z1], cmap=plt.get_cmap(self.cmap_image), \n",
    "                vmin=self.v[0], vmax=self.v[1])\n",
    "            ax[0][1].imshow(self.vol2[:,:,z2], cmap=plt.get_cmap(self.cmap_image), \n",
    "                vmin=self.v[0], vmax=self.v[1])\n",
    "            ax[0][2].imshow(self.vol3[:,:,z3], cmap=plt.get_cmap(self.cmap_image), \n",
    "                vmin=self.v[0], vmax=self.v[1])\n",
    "            \n",
    "            ax[1][0].imshow(self.vol1_label[:,:,z1], cmap=plt.get_cmap(self.cmap_label),\n",
    "                vmin=self.v_label[0], vmax=self.v_label[1])\n",
    "            ax[1][1].imshow(self.vol2_label[:,:,z2], cmap=plt.get_cmap(self.cmap_label),\n",
    "                vmin=self.v_label[0], vmax=self.v_label[1])\n",
    "            ax[1][2].imshow(self.vol3_label[:,:,z3], cmap=plt.get_cmap(self.cmap_label),\n",
    "                vmin=self.v_label[0], vmax=self.v_label[1])\n",
    "            \n",
    "            ax[2][0].imshow(self.vol1_pred_pnet[:,:,z1], cmap=plt.get_cmap(self.cmap_pred_pnet),\n",
    "                vmin=self.v_pred_pnet[0], vmax=self.v_pred_pnet[1], alpha = 0.3)\n",
    "            ax[2][1].imshow(self.vol2_pred_pnet[:,:,z2], cmap=plt.get_cmap(self.cmap_pred_pnet),\n",
    "                vmin=self.v_pred_pnet[0], vmax=self.v_pred_pnet[1], alpha = 0.3)\n",
    "            ax[2][2].imshow(self.vol3_pred_pnet[:,:,z3], cmap=plt.get_cmap(self.cmap_pred_pnet),\n",
    "                vmin=self.v_pred_pnet[0], vmax=self.v_pred_pnet[1], alpha = 0.3)\n",
    "\n",
    "            ax[3][0].imshow(self.vol1_pred_rnet[:,:,z1], cmap=plt.get_cmap(self.cmap_pred_rnet),\n",
    "                vmin=self.v_pred_rnet[0], vmax=self.v_pred_rnet[1], alpha = 0.3)\n",
    "            ax[3][1].imshow(self.vol2_pred_rnet[:,:,z2], cmap=plt.get_cmap(self.cmap_pred_rnet),\n",
    "                vmin=self.v_pred_rnet[0], vmax=self.v_pred_rnet[1], alpha = 0.3)\n",
    "            ax[3][2].imshow(self.vol3_pred_rnet[:,:,z3], cmap=plt.get_cmap(self.cmap_pred_rnet),\n",
    "                vmin=self.v_pred_rnet[0], vmax=self.v_pred_rnet[1], alpha = 0.3)\n",
    "            \n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1483664fffad4b798df5a9b878c8d3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ImageSliceViewer3D_with_prediction at 0x7f468fec9310>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnet_inferences = glob.glob(f\"{save_dir}/*_pred_pnet.nii.gz\")\n",
    "rnet_inferences = glob.glob(f\"{save_dir}/*_pred_rnet.nii.gz\")\n",
    "transform = tio.Compose([\n",
    "    tio.ToCanonical(), \n",
    "    tio.Resample(1), \n",
    "    tio.RemapLabels({2:1, 3:1, 4:1})\n",
    "])\n",
    "\n",
    "idx = 0\n",
    "\n",
    "image_path, label_path, pnet_inf_path, rnet_inf_path = (test_images[idx], \n",
    "                                                        test_labels[idx],\n",
    "                                                        pnet_inferences[idx], \n",
    "                                                        rnet_inferences[idx])\n",
    "\n",
    "test_subject = tio.Subject(\n",
    "    image = tio.ScalarImage(image_path),\n",
    "    label = tio.LabelMap(label_path),\n",
    "    pnet_inf = tio.LabelMap(pnet_inf_path),\n",
    "    rnet_inf = tio.LabelMap(rnet_inf_path)\n",
    ")\n",
    "test_subject = transform(test_subject)\n",
    "\n",
    "image_np = test_subject.image.data.squeeze(dim=0).numpy()\n",
    "label_np = test_subject.label.data.squeeze(dim=0).numpy()\n",
    "pnet_inf_np = test_subject.pnet_inf.data.squeeze(dim=0).numpy()\n",
    "rnet_inf_np = test_subject.rnet_inf.data.squeeze(dim=0).numpy()\n",
    "\n",
    "ImageSliceViewer3D_with_prediction(image_np, label_np, pnet_inf_np, rnet_inf_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06b312d4da2a9a3686a6d52820f5105a519faf7cd6cc067e3b3e5e11d5973e41"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ys_torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
